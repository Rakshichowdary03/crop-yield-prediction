{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6edcec21",
   "metadata": {},
   "source": [
    "\n",
    "# Crop Yield Prediction — Colab Notebook\n",
    "This notebook trains **Linear Regression**, **Random Forest**, and **XGBoost** models on a crop yield dataset and reports **R²** and **RMSE**. It mirrors the steps in your PDF.\n",
    "\n",
    "> **How to use:** Open this notebook in **Google Colab** and run each cell from top to bottom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eed9402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install dependencies (Colab-friendly)\n",
    "!pip install -q xgboost scikit-learn pandas matplotlib joblib pytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698356b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n",
    "print(\"Libraries imported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98721c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "# Primary: Public dataset (YBI Foundation)\n",
    "# Fallback: Generate a small synthetic dataset if the URL isn't reachable.\n",
    "\n",
    "DATA_URL = \"https://raw.githubusercontent.com/ybifoundation/Dataset/main/Crop%20Yield.csv\"\n",
    "\n",
    "def load_data():\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_URL)\n",
    "        print(\"Loaded dataset from URL:\", DATA_URL)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load remote dataset. Reason:\", e)\n",
    "        print(\"Falling back to a small synthetic dataset (for demo).\")\n",
    "        rng = np.random.default_rng(42)\n",
    "        n = 300\n",
    "        df = pd.DataFrame({\n",
    "            \"Temperature\": rng.normal(26, 4, n),\n",
    "            \"Rainfall\": rng.normal(900, 150, n),\n",
    "            \"Soil_Type\": rng.choice([\"Sandy\", \"Loam\", \"Clay\"], size=n),\n",
    "            \"Fertilizer\": rng.normal(80, 15, n),\n",
    "            \"Pesticide\": rng.normal(5, 1.5, n),\n",
    "        })\n",
    "        # Nonlinear relation + noise (toy)\n",
    "        soil_bonus = df[\"Soil_Type\"].map({\"Sandy\":-2, \"Loam\":3, \"Clay\":1}).astype(float)\n",
    "        df[\"Crop_Yield\"] = (\n",
    "            0.5*df[\"Temperature\"]\n",
    "            + 0.01*df[\"Rainfall\"]\n",
    "            + 0.2*df[\"Fertilizer\"]\n",
    "            - 0.1*df[\"Pesticide\"]\n",
    "            + soil_bonus\n",
    "            + rng.normal(0, 2.5, n)\n",
    "        )\n",
    "        return df\n",
    "\n",
    "df = load_data()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode categorical variables and split\n",
    "le = LabelEncoder()\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "X = df.drop('Crop_Yield', axis=1)\n",
    "y = df['Crop_Yield']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e758db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=200, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=300, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_name, best_score, best_model = None, -np.inf, None\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    results.append((name, r2, rmse))\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    if r2 > best_score:\n",
    "        best_score, best_name, best_model = r2, name, model\n",
    "\n",
    "print(f\"\\nBest model: {best_name} (R²={best_score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save best model\n",
    "joblib.dump(best_model, \"model.pkl\")\n",
    "print(\"Saved best model to model.pkl\")\n",
    "\n",
    "# Simple feature importance/coefficients plot (if available)\n",
    "def plot_importances(model, feature_names):\n",
    "    values = None\n",
    "    title = \"Feature Importance\"\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        values = model.feature_importances_\n",
    "    elif hasattr(model, \"coef_\"):\n",
    "        coef = getattr(model, \"coef_\")\n",
    "        values = np.abs(coef) if np.ndim(coef)==1 else np.abs(coef).mean(axis=0)\n",
    "        title = \"Absolute Coefficients\"\n",
    "    if values is None:\n",
    "        print(\"This model doesn't expose importances/coefficients.\")\n",
    "        raise SystemExit\n",
    "\n",
    "    order = np.argsort(values)[::-1]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.bar(range(len(values)), values[order])\n",
    "    plt.xticks(range(len(values)), np.array(feature_names)[order], rotation=45, ha=\"right\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_importances(best_model, X_train.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcadb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick 'tests' inline (simulate CI checks)\n",
    "assert df.isnull().sum().sum() == 0, \"Data contains nulls!\"\n",
    "print(\"Test passed: No null values in dataset.\")\n",
    "\n",
    "# Check minimum performance (soft check; won't fail hard if using synthetic data)\n",
    "try:\n",
    "    # On the real dataset, this should be reasonably high; synthetic may vary\n",
    "    assert best_score > 0.70, f\"R² too low: {best_score:.2f}\"\n",
    "    print(\"Test passed: R² above threshold.\")\n",
    "except AssertionError as e:\n",
    "    print(\"Warning:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6adf4b",
   "metadata": {},
   "source": [
    "\n",
    "### Notes\n",
    "- When you run this in Colab, the dataset will be fetched from the public URL.  \n",
    "- If the URL is temporarily unavailable, the notebook will **fall back to a synthetic dataset** so you can still see outputs.\n",
    "- The trained best model is saved as `model.pkl` in the Colab runtime.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
